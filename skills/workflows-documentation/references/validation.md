# Validation - Quality Assessment and Gates

Comprehensive reference for qualitative quality assessment, gating, and improvement recommendations for markdown documentation using script-assisted AI analysis.

---

## 1. ðŸ“– INTRODUCTION & PURPOSE

### What Is Validation?

Validation provides a comprehensive quality assessment framework for markdown documentation. The `extract_structure.py` script parses documents into structured JSON; AI uses this data to evaluate quality across structure, content, and style dimensions.

**Core Purpose**:
- **Quality assessment** - AI-evaluated judgement across structure, content, and style
- **Quality gates** - Document-specific thresholds for production readiness
- **Improvement guidance** - Actionable recommendations based on AI analysis
- **Checklist validation** - Automated structural checks via script

**Progressive Disclosure Context**:
```
Level 1: SKILL.md metadata (name + description)
         â””â”€ Always in context (~100 words)
            â†“
Level 2: SKILL.md body
         â””â”€ When skill triggers (<5k words)
            â†“
Level 3: Reference files (this document)
         â””â”€ Loaded as needed for assessment details
```

This reference file provides Level 3 deep-dive technical guidance on qualitative assessment, gates, and interpretation patterns.

### Core Principle

**"Measure what matters, gate what guarantees quality"** - Structure ensures validity, Content ensures AI-friendliness, Style ensures consistency. AI evaluates all dimensions based on `extract_structure.py` output.

---

## 2. ðŸ“Š SCRIPT-ASSISTED AI EVALUATION

**Validation uses `extract_structure.py` to parse documents, then AI evaluates:**

```
INPUT: Markdown Document
    â†“
SCRIPT: extract_structure.py
    - Parses frontmatter (values + issues)
    - Extracts structure (headings, sections, code blocks)
    - Calculates metrics (word count, heading depth, code ratio)
    - Runs type-specific checklist
    - Generates evaluation questions
    â†“
OUTPUT: Structured JSON
    â†“
AI EVALUATION:
    - Reviews checklist results
    - Answers evaluation questions
    - Assesses content quality
    - Provides recommendations
    â†“
RESULT: Quality Assessment + Recommendations
```

---

## 3. ðŸ”¢ CHECKLIST-BASED VALIDATION

### Structure Checklist (from extract_structure.py)

The script runs type-specific checklists and reports pass/fail results:

**Common Checks** (all document types):
- âœ… Single H1, no duplicates
- âœ… Proper heading hierarchy (no skipped levels)
- âœ… Code blocks properly fenced
- âœ… No unclosed markdown elements
- âœ… Section separators (`---`) correct
- âœ… Emoji usage correct (H2 numbered have emoji, H3 semantic only in RULES sections)

**SKILL-Specific Checks**:
- âœ… YAML frontmatter present and valid
- âœ… Required fields: name, description
- âœ… Description on single line (no YAML multiline)
- âœ… allowed-tools in array format
- âœ… Required sections present (WHEN TO USE, HOW TO USE, RULES)

**README-Specific Checks**:
- âœ… No frontmatter (or optional)
- âœ… Clear purpose/introduction section
- âœ… Usage examples present

**Checklist Results** in JSON:
```json
{
  "checklist": {
    "results": [
      {"check": "single_h1", "status": "pass"},
      {"check": "frontmatter_valid", "status": "pass"},
      {"check": "description_single_line", "status": "fail", 
       "message": "Description uses multiline format"}
    ],
    "passed": 8,
    "failed": 1,
    "total": 9
  }
}
```

### Content Quality (AI-Evaluated)

AI evaluates content based on extracted data and evaluation questions:

**Evaluation Questions** (generated by script):
- Is the purpose clear from the introduction?
- Are examples practical and complete?
- Is information well-organized and scannable?
- Are there any gaps in coverage?

**AI Assessment Criteria**:
- Clarity and completeness
- Practical usefulness
- AI-friendliness (scannable, question-answering format)
- Appropriate level of detail

### Style Compliance (AI-Evaluated)

AI checks style based on core_standards.md:

**Key Style Checks**:
- âœ… H2 numbered headings: ALL CAPS + emoji (e.g., `## 1. ðŸŽ¯ WHEN TO USE`)
- âœ… H3 headings: Only semantic emojis (âœ… âŒ âš ï¸), no decorative emojis
- âœ… Code examples include comments
- âœ… Bullet lists under 7 items
- âœ… Consistent terminology
- âœ… Active voice preferred

**Emoji style compliance**:
- âœ… Semantic emojis on H3: `### âœ… ALWAYS Rules` (functional signal)
- âŒ Decorative emojis on H3: `### ðŸ”§ Pattern 1` (visual noise)
- See [core_standards.md](./core_standards.md#6--emoji-usage-rules) for criteria

---

## 4. ðŸ§® DOCUMENT QUALITY INDEX (DQI)

The `extract_structure.py` script computes a **Document Quality Index (DQI)** - a 100% deterministic score from 0-100 based on measurable document attributes. This replaces subjective AI-only assessment with quantifiable metrics.

### DQI Components

| Component | Max | What It Measures |
|-----------|-----|------------------|
| **Structure** | 40 | Checklist pass rate (type-specific validation) |
| **Content** | 30 | Word count, heading density, code examples, tables/lists, links |
| **Style** | 30 | H2 formatting (number+emoji+CAPS), section dividers, intro paragraph |

### Content Score Breakdown (30 points)

| Metric | Max | Criteria |
|--------|-----|----------|
| Word count | 10 | Within type-specific range (e.g., skills: 2000-8000 words) |
| Heading density | 8 | Appropriate H2 count per 500 words |
| Code examples | 6 | 3+ code blocks with language tags = full points |
| Tables/lists | 3 | Presence of tables (+2) and lists (+1) |
| Links | 3 | Internal links (+2) and external links (+1) |

### Style Score Breakdown (30 points)

| Metric | Max | Criteria |
|--------|-----|----------|
| H2 formatting | 12 | Number + emoji + ALL CAPS on H2 headings |
| Section dividers | 6 | Horizontal rules between H2 sections |
| Style issues | 8 | Penalty of -2 per style issue detected |
| Intro paragraph | 4 | Brief introduction after H1 |

### Quality Bands

| Band | Score Range | Status | Action |
|------|-------------|--------|--------|
| **Excellent** | 90-100 | âœ… Production-ready | None needed |
| **Good** | 75-89 | âœ… Shareable | Minor improvements recommended |
| **Acceptable** | 60-74 | âš ï¸ Functional | Several areas need attention |
| **Needs Work** | <60 | âŒ Not ready | Significant improvements required |

### DQI JSON Output

```json
{
  "dqi": {
    "total": 96,
    "band": "excellent",
    "band_description": "Production-ready documentation",
    "components": {
      "structure": 40,
      "structure_max": 40,
      "content": 26,
      "content_max": 30,
      "style": 30,
      "style_max": 30
    },
    "breakdown": {
      "checklist_pass_rate": 100.0,
      "word_count": 4168,
      "word_count_score": 10,
      "h2_count": 9,
      "heading_score": 8,
      "code_block_count": 8,
      "code_score": 6,
      "h2_format_score": 12,
      "style_issue_count": 0
    }
  }
}
```

### Key Principles

1. **100% Deterministic**: Every point is computed from measurable data, not AI judgment
2. **Transparent**: Full breakdown shows exactly where points are earned/lost
3. **Type-Aware**: Thresholds adjust based on document type (skill, reference, asset, etc.)
4. **Actionable**: Low scores in specific components guide improvement priorities

---

## 5. ðŸ”’ QUALITY GATES

### Assessment Scale

| Level | Status | Checklist | Content | Action |
| --- | --- | --- | --- | --- |
| **Excellent** | âœ… Production ready | No failures | High quality | None needed |
| **Good** | âœ… Shareable | Minor issues | Good quality | Optional improvements |
| **Acceptable** | âš ï¸ Functional | Some issues | Functional | Optimization recommended |
| **Needs Work** | âŒ Not ready | Critical failures | Issues | Fix required |

### Document-Specific Requirements

**SKILL.md**:
- Checklist: No failures allowed (strict)
- Content: Highly AI-friendly required
- Style: Exemplary compliance
- Expected: Excellent

**Knowledge**:
- Checklist: No failures allowed (strict, no frontmatter)
- Content: Good AI-friendliness
- Style: Consistent formatting
- Expected: Good

**README**:
- Checklist: Minor issues acceptable (flexible)
- Content: Highly AI-friendly
- Style: Good compliance
- Expected: Good

**Command**:
- Checklist: No failures allowed (strict)
- Content: Functional coverage
- Style: Consistent formatting
- Expected: Acceptable+

**Spec**:
- Checklist: Issues acceptable (loose, working doc)
- Content: N/A (development document)
- Style: Basic compliance
- Expected: Acceptable

---

## 6. ðŸ“ˆ ASSESSMENT INTERPRETATION

### Checklist Passes, Low Content Quality

**Diagnosis**: Valid structure but content needs improvement

**Fix**: AI will recommend content improvements:
- Add question-answering snippets
- Combine installation with usage examples
- Provide complete workflow examples
- Add practical context

### Good Content, Checklist Failures

**Diagnosis**: Good content but structural issues

**Fix**: Address checklist failures first:
- Fix frontmatter issues (single-line description, array format)
- Fix heading hierarchy
- Add missing required sections
- Close unclosed elements

### Style Issues

**Diagnosis**: Valid structure, good content, but inconsistent formatting

**Fix**: Apply style guide compliance:
- Fix H2 heading format (ALL CAPS + emoji)
- Add code comments
- Break long bullet lists (max 7 items)
- Use consistent terminology

### Multiple Issues

**Diagnosis**: Both checklist failures and content issues

**Fix**: Address in order:
1. Fix critical checklist failures first
2. Improve content quality
3. Apply style fixes last

---

## 7. ðŸ’¡ IMPROVEMENT RECOMMENDATIONS

**When Quality Rating < 80**:

1. **Review checklist results** - Fix failures first
2. **Read evaluation questions** - Address gaps
3. **Apply AI recommendations** - Follow suggested improvements
4. **Re-extract and evaluate** - Check improvement
5. **Iterate** - Repeat until threshold met

**Priority order**:
1. Critical checklist failures - Must be fixed
2. Content quality issues - Primary improvement target
3. Style compliance - Polish for consistency

**Quick fixes** (high impact, low effort):
- Structure: Fix frontmatter format, add missing sections
- Content: Add examples, combine concepts with usage
- Style: Fix H2 format, add emoji, break long lists

---

## 8. ðŸ“‹ JSON OUTPUT FORMAT

**Example extract_structure.py output**:
```json
{
  "file": "specs/042/spec.md",
  "type": "spec",
  "frontmatter": {
    "parsed": {"title": "Feature Spec"},
    "issues": []
  },
  "structure": {
    "headings": [
      {"level": 1, "text": "Feature Spec", "line": 5},
      {"level": 2, "text": "1. Overview", "line": 10}
    ],
    "sections": [...],
    "code_blocks": [...]
  },
  "metrics": {
    "word_count": 1250,
    "heading_count": 8,
    "max_heading_depth": 3,
    "code_block_count": 4,
    "code_ratio": 0.15
  },
  "checklist": {
    "results": [
      {"check": "single_h1", "status": "pass"},
      {"check": "heading_hierarchy", "status": "pass"},
      {"check": "code_blocks_fenced", "status": "pass"}
    ],
    "passed": 6,
    "failed": 0,
    "total": 6
  },
  "evaluation_questions": [
    "Is the purpose clear from the introduction?",
    "Are there practical examples for key concepts?",
    "Is the document well-organized and scannable?"
  ]
}
```

**AI then provides assessment**:
```
=== Quality Assessment: specs/042/spec.md ===

Document Type: spec (loose enforcement)

CHECKLIST RESULTS: 6/6 passed âœ…

CONTENT EVALUATION:
âœ“ Purpose clear from introduction
âœ“ Good heading structure
âš  Could use more practical examples
âš  Some sections could be more scannable

STYLE NOTES:
âœ“ H2 format correct
âš  Some bullet lists exceed 7 items

OVERALL: Good (85) - Ready for use

RECOMMENDATIONS:
1. Add integration examples
2. Break long bullet lists
```

---

## 9. ðŸ’» VALIDATION COMMANDS

**Extract structure for single file**:
```bash
scripts/extract_structure.py document.md
# Outputs JSON to stdout for AI evaluation
```

**Quick validation** (skill folders):
```bash
scripts/quick_validate.py .opencode/skills/my-skill
# Fast check for essential requirements
```

**Quick validation with JSON output**:
```bash
scripts/quick_validate.py .opencode/skills/my-skill --json
# Machine-readable output for automation
```

**Batch extraction**:
```bash
for file in $(find specs/ -name "*.md"); do
  echo "=== $file ==="
  scripts/extract_structure.py "$file"
done
```

**CI/CD integration** (example):
```bash
# Extract structure and check for critical failures
output=$(scripts/extract_structure.py README.md)
if echo "$output" | grep -q '"status": "fail"'; then
  echo "Checklist failures detected"
  exit 1
fi
```

---

## 10. ðŸ”— RELATED RESOURCES

### Reference Files
- [core_standards.md](./core_standards.md) - Document type rules and structural requirements
- [workflows.md](./workflows.md) - Execution modes and workflow details
- [optimization.md](./optimization.md) - Content transformation patterns
- [quick_reference.md](./quick_reference.md) - Quick command reference

### Templates
- [skill_md_template.md](../assets/skill_md_template.md) - SKILL.md file templates
- [skill_asset_template.md](../assets/skill_asset_template.md) - Bundled asset structure
- [readme_template.md](../assets/readme_template.md) - Comprehensive README guide (13 sections)
- [command_template.md](../assets/command_template.md) - Command creation guide (19 sections)
- [install_guide_template.md](../assets/install_guide_template.md) - Install guide template (14 sections)
- [llmstxt_templates.md](../assets/llmstxt_templates.md) - llms.txt with decision framework
- [frontmatter_templates.md](../assets/frontmatter_templates.md) - Frontmatter validation & templates (11 sections)